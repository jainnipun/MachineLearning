{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModelling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jainnipun/MachineLearning/blob/master/TextAnalytics/TopicModelling..ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PQ1BD7XCmqxw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pd.set_option(\"display.max_colwidth\", 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SOVRHZdnm-_T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=1, random_state =1, remove=('headers','footers','quotes'))\n",
        "documents = dataset.data\n",
        "#len(documents)\n",
        "#datasets.taget_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vdNiudthoCMi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "\n",
        "# removing everything except alphabets`\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n",
        "# removing short words\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "# make all text lowercase\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rkBTNC02pa_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'documents':documents})\n",
        "\n",
        "#Removing everything except columns\n",
        "news_df['clean_doc'] = news_df[\"documents\"].str.replace(\"[^a-zA-Z#]\",\" \");\n",
        "\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x:x.lower())\n",
        "\n",
        "news_df['clean_doc']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PD5x2v7kpksp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "news_df['clean_doc']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whzlh-gprusm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GRplNgl9rKJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VruvcIbZr9n-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tokenization\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x:x.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZPyuXN9sOZF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_doc = tokenized_doc.apply(lambda x: [w for w in x if w not in stop_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PItOu8Brta3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#detokenization\n",
        "detokenized_doc = []\n",
        "for i in range(len(tokenized_doc)): # or len(news_df['clean_doc'])\n",
        "  detokenized = ' '.join(tokenized_doc[i])\n",
        "  detokenized_doc.append(detokenized)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90jautBpwXRl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "max_features= 1000, # keep top 1000 terms \n",
        "max_df = 0.5, \n",
        "smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "\n",
        "X.shape # check shape of the document-term matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "11OzjX9YRGoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# SVD represent documents and terms in vectors \n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "\n",
        "svd_model.fit(X)\n",
        "\n",
        "len(svd_model.components_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vsu9_9MzRMVu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
        "    print(\"Topic \"+str(i)+\": \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0])\n",
        "        print(\" \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5IJwN2NRO49",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "X_topics = svd_model.fit_transform(X)\n",
        "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
        "c = dataset.target,\n",
        "s = 10, # size\n",
        "edgecolor='none'\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}