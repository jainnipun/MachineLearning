{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Semantic Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jainnipun/MachineLearning/blob/master/TextAnalytics/NLP_Semantic_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Fq6mX_ArzGIP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP :  Use Stopword\n",
        "\n",
        "**How  to use default Stopwords corpus present in Natural Language Toolkit (NLTK).**\n",
        "\n",
        "\n",
        "**Stopwords are the frequently occurring words in a text document. For example, a, the, is, are, etc**"
      ]
    },
    {
      "metadata": {
        "id": "p61mF3qb0i9B",
        "colab_type": "code",
        "outputId": "eaef6e87-559c-4faf-f839-1f96697ab78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "y5MWuQ8o0Rly",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading stopwords Corpus**"
      ]
    },
    {
      "metadata": {
        "id": "E4XZli3YzKsp",
        "colab_type": "code",
        "outputId": "e06111d0-32dd-4a8a-aa00-b86219bf85ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords \n",
        "print (stopwords.fileids())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gtu2PpRAzK0D",
        "colab_type": "code",
        "outputId": "7bc78563-0acf-4508-cadc-80b424248517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "print ('English Stopwords : ',stopwords.words('english'))\n",
        "print ('French Stopwords : ',stopwords.words('french'))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Stopwords :  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "French Stopwords :  ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'je', 'la', 'le', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kxxrr0cF1FOY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tokenize Words**\n",
        "\n",
        "We split the text sentence/paragraph into a list of words. Each word in the list is called a token."
      ]
    },
    {
      "metadata": {
        "id": "1URfdVd0zK3a",
        "colab_type": "code",
        "outputId": "8a034c9d-bd51-4c5c-ebb4-ee27b2a84424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('punkt') \n",
        "text = \"Hello my name is Nipun Jain. I am a Machine Learning enthusiast.\"\n",
        " \n",
        "# Normalize text\n",
        "# NLTK considers capital letters and small letters differently.\n",
        "# For example: Tree and tree are considered as two different words.\n",
        "# Hence, we convert all letters of our text into lowercase.\n",
        "text = text.lower()\n",
        " \n",
        "# tokenize text \n",
        "words = word_tokenize(text)\n",
        " \n",
        "print (words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['hello', 'my', 'name', 'is', 'nipun', 'jain', '.', 'i', 'am', 'a', 'machine', 'learning', 'enthusiast', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6tleiPWs1dza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RubURhLuzK6Y",
        "colab_type": "code",
        "outputId": "293103cd-bdf0-463e-8874-f3c84a4cc5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "words = [w for w in words if w.isalpha()]\n",
        "print(words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'my', 'name', 'is', 'nipun', 'jain', 'i', 'am', 'a', 'machine', 'learning', 'enthusiast']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w8Flypv614Ar",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Removing Stop Words**\n",
        "\n",
        "Here, we will remove stop words from our text data using the default stopwords corpus present in NLTK.\n",
        "\n",
        "Get list of English Stopwords"
      ]
    },
    {
      "metadata": {
        "id": "mjX89UHGzK9a",
        "colab_type": "code",
        "outputId": "0335076b-6461-4db3-e7e8-51b12e49edcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        " \n",
        "print ('Length of Stopwords',len(stop_words))\n",
        " \n",
        "words_filtered = words[:] # creating a copy of the words list\n",
        " \n",
        "for word in words:\n",
        "    if word in stop_words:        \n",
        "        words_filtered.remove(word)\n",
        " \n",
        "print (words_filtered)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Stopwords 179\n",
            "['hello', 'name', 'nipun', 'jain', 'machine', 'learning', 'enthusiast']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfRRalgm2DM9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Updating Stop Words Corpus**\n",
        "\n",
        "Suppose, you don’t want to omit some stopwords for your text analysis. In such case, you have to remove those words from the stopwords list.\n",
        "\n",
        "Let’s suppose, you want the words over and under for your text analysis. The words “my”  are present in the stopwords corpus by default.\n",
        "Let’s remove them from the stopwords corpus."
      ]
    },
    {
      "metadata": {
        "id": "EQVH6AHu2BwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "XAuxhy5R2CH3",
        "colab_type": "code",
        "outputId": "1ca67c9a-41c9-4ddf-83b5-7c3f8cad03d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# set() function removes entries from the list . Here we removed 'my' from stopwords\n",
        "stop_words = set(stopwords.words('english')) - set(['my'])\n",
        "print ('Length of Stopwords',len(stop_words))\n",
        "print (stop_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Stopwords 178\n",
            "{'at', 'so', 'they', 'not', 'me', 'yourself', 'too', 'why', 'himself', \"wasn't\", 'has', 'both', 'aren', 'here', 'but', \"hadn't\", 'during', 'you', 'which', 'is', 'about', 's', \"haven't\", 'd', 'his', 'i', 'did', 'ours', 'doesn', 'into', \"shan't\", 'each', 'can', 'are', 'nor', 'if', 'this', 'don', 'through', 'hasn', 'because', \"you'd\", 'couldn', 'again', \"weren't\", 'out', \"shouldn't\", 'only', \"it's\", 'few', 'yours', 'he', 'any', \"she's\", 'hers', 'and', 've', 'most', 'or', 'were', 'for', 'from', 'no', 'didn', 'm', 'mustn', 'itself', 'shan', 'we', 'as', \"that'll\", 'with', 'above', \"won't\", 'under', 'when', 'them', 'herself', 'should', \"you're\", 'off', \"aren't\", 'was', \"didn't\", 'wasn', 'mightn', 'its', 'your', 'up', \"you'll\", 'same', 'y', 'that', 'own', \"mightn't\", 'before', 'these', 'their', \"doesn't\", \"mustn't\", 'our', 'over', 'ain', 'those', 'other', 'hadn', \"needn't\", 'being', \"wouldn't\", \"you've\", 'on', 'isn', 'than', 'further', 'ma', 'all', 'her', 'it', 'needn', 'such', 'shouldn', 'do', 'more', 't', \"should've\", 'will', 'down', 'him', 'where', 'am', \"couldn't\", 'against', 'between', 'what', 'a', 'by', 'of', 'whom', 'o', \"don't\", 'won', 'then', 'haven', 'below', 'the', 'been', 'have', 'weren', 'having', 'while', 're', 'some', 'themselves', 'now', 'had', 'once', 'just', 'theirs', 'in', 'there', \"hasn't\", 'she', 'wouldn', 'll', 'ourselves', 'after', 'very', 'doing', 'to', 'yourselves', 'until', \"isn't\", 'an', 'who', 'how', 'does', 'myself', 'be'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4ZqhopkQ2Rfj",
        "colab_type": "code",
        "outputId": "2920a925-5cfa-439b-ea52-6d680cb04141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "words_filtered = words[:] # creating a copy of the words list\n",
        " \n",
        "for word in words:\n",
        "    if word in stop_words:        \n",
        "        words_filtered.remove(word)\n",
        " \n",
        "print (words_filtered)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'my', 'name', 'nipun', 'jain', 'machine', 'learning', 'enthusiast']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}